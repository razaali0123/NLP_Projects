{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2020 - Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sophia Student, 1234567* (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, December 16, 2020, 10am</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE**\n",
    "<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-4.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle. In case of questions, you can contact the TAs or David via the usual channels.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this assignment, you will implement a feedforward neural network and train it with backpropagation to classify intent from the provided dataset (<https://github.com/Dark-Sied/Intent_Classification>). For the purpose of understanding the learning process, the whole dataset is used as both training and test data. (What does that mean for your results?)<br><br>\n",
    "\n",
    "You should implement all part of this exercise using only python + standard library + NumPy. (That is, no specialised machine learning libraries are allowed.) Here is a list of NumPy functions that may or may not be useful for this task: <br>\n",
    "`np.array(), np.eye(), np.reshape(), np.ones(), np.zeros(), np.dot(), np.concatenate(), np.maximum(), np.argmax(), np.sum(), np.uniform()`. <br><br>\n",
    "\n",
    "A more comprehensive introduction to NumPy can be found here: <https://sites.engineering.ucsb.edu/~shell/che210d/numpy.pdf> .\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For your convenience, a function for reading in the dataset:\n",
    "import csv\n",
    "\n",
    "def load_dataset(filename):\n",
    "    intent = []\n",
    "    unique_intent = []\n",
    "    sentences = []\n",
    "    with open(filename, \"r\", encoding=\"latin1\") as f:\n",
    "        data = csv.reader(f, delimiter=\",\")\n",
    "        for row in data:\n",
    "            sentences.append(row[0])\n",
    "            intent.append(row[1])\n",
    "    unique_intent = set(intent)\n",
    "    return sentences, intent, unique_intent\n",
    "            \n",
    "sentences, intent, unique_intent = load_dataset(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Bag-of-Words Representation [15pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The first thing you're being asked to do is to convert the text into a bag-of-words representation matrix where the dimension of the matrix is $V$ x $M$ ($M$: number of examples, $V$: vocabulary size) and the label to a matrix of dimension $K$ x $M$ where $K$ is number of classes.   \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Activation Function [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "For the classification task, the softmax activation function for the output layer with K classes is given by: \n",
    "$softmax(z_i) = \\frac{e^{z_i}}{{\\sum_{j=1}^{K}e^{z_j}}}$ <br>\n",
    "The activation function of the hidden neurons is a non-linear function. We have seen tanh being used in class, but more common these days are for example ReLU or sigmoid, given by: <br>\n",
    "$ReLU(z)=max(0,z)$ <br>\n",
    "$sigmoid(z)=\\frac{1}{1+e^{-z}}$ <br>\n",
    "\n",
    "Implement the softmax, ReLU, and sigmoid activation function in such a way that it accepts NumPy array and matrices. Plot the ReLU and sigmoid functions, as well as their derivatives. Observe the plot and discuss briefly what the advantages and disadvantages of the ReLU and sigmoid activation function might be. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Feedforward Neural Network [35 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Now that you have created the input matrix, we can implement our neural network and perform a forward propagation to classify intent. To perform the forward propagation, you should compute $z^{l}$ and pass it through the activation function for each layer, given by: <br><br>\n",
    "$z^{l} = W^{l}a^{l-1} + b^{l}$ <br>\n",
    "$a^{l} = g(z^{l})$ <br>\n",
    "where $W^{l}$ is a weight matrix between layer $l$ and $l+1$, $z^{l}$ is value of the hidden layer at layer $l$ before activation, $a^{l}$ is value of the hidden layer at layer $l$ after activation, and $b^{l}$ is bias term for layer $l$.\n",
    "\n",
    "You should implement the feedforward computation that computes $\\hat{y_{i}}$ for every example $i$. The neural network has 3 layers - an input layer, a hidden layer and an output layer, where the hidden layer has 150 neurons. Don't forget to include the bias term. Use ReLU as the activation function for the hidden layer and softmax for the output layer. For parameters initialization, use random values from uniform distribution in the range (-1,1). Provide a seed value to the random number generator, to make the results reproducible. The purpose of using this kind of initialisation is to break symmetry and ensure that different neurons can learn different non-linear functions. (Hint: use vectorization methods instead of a for loop for speedup.) <br><br>\n",
    "\n",
    "Use this neural network to predict the intent and calculate the accuracy of the classifier. (Should you be expecting high numbers yet?)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Backpropagation [35 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "You will now implement the backpropagation algorithm to compute the gradient of the cost function with respect to the neural network weights' and bias term.  First of all, implement the cross entropy loss function to monitor if your model is actually learning. Remember that in backpropagation, we want to propagate the error signal to measure how much each neuron in the hidden layer contributes to the error in the output layer. It is more or less similar to forward propagation but in a reverse direction. For the output layer, set $\\delta$ for cross entropy loss: <br><br>\n",
    "$\\delta^{L}= \\hat{y} - y$ <br> where $L$ is the output layer and $\\hat{y}$ is prediction of $y$. <br>\n",
    "\n",
    "For the remaining hidden layer $l$, set: <br><br>\n",
    "$\\delta^{l} = (W^{l})^{T}\\delta^{l+1} \\odot g'(z^{l})$ <br> where $\\odot$ is an element-wise product of matrices (Hadamard product), $g'$ is the derivative of the activation function. <br>\n",
    "\n",
    "The derivative of the ReLU is given by:  $ReLU'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\\n",
    "                                                                                                                                      0 & \\text{otherwise}.\\end{cases}$<br>\n",
    "\n",
    "By calculating the error term for each layer, you can then use the error term to calculate the partial derivatives $\\frac{\\partial \\mathcal{L}}{\\partial W^{l}} = \\delta_{l+1} (a^{l})^{T}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b^{l}} = \\delta_{l+1}$ and perform batch gradient descent to update the parameter. (Batch gradient descent = run through all training instances and compute the gradient, then make the weight update.) Make sure that you accumulate the gradients for all the training samples and divide it by number of samples before doing the update. <br><br>\n",
    "\n",
    "Here is some simple pseudocode to help with the training procedure: <br>\n",
    " * for number of epoch:\n",
    "    > define gradient accumulator $\\Delta w=0, \\Delta b=0$ for each weight and bias term <br>\n",
    "    > define cost accumulator $\\Delta \\mathcal{L}=0$ for the loss <br>\n",
    "    \n",
    "    > for each training example $i$:<br>\n",
    "        >> perform forward propagation <br>\n",
    "        >> calculate loss on example $i, L_{i}$ <br>\n",
    "        >> $\\Delta \\mathcal{L} = \\Delta \\mathcal{L} + L_{i}$ <br> <br> \n",
    "        >> perform backpropagation <br>\n",
    "        >> $\\Delta w = \\Delta w + \\frac{\\partial \\mathcal{L}}{\\partial W}$ for each weight <br>\n",
    "        >> $\\Delta b = \\Delta b + \\frac{\\partial \\mathcal{L}}{\\partial b}$ for each bias term <br> \n",
    "        \n",
    "    > calculate the cost, which is just average loss ($Cost = \\frac{1}{m}\\Delta \\mathcal{L}$) <br>\n",
    "    > $w = w - \\frac{\\alpha}{m}\\Delta w$ for each weight <br>\n",
    "    > $b = b - \\frac{\\alpha}{m}\\Delta b$ for each bias term <br> \n",
    "\n",
    "Run the training for 1000 epoch using learning rate = 0.005 and use this neural network to predict the intent and calculate the accuracy of the classifier. (Hint: the dimension of $\\delta^{l}$ should match the dimension of $a^{l}$, and the dimension of $\\frac{\\partial \\mathcal{L}}{\\partial W^{l}}$ and $\\frac{\\partial \\mathcal{L}}{\\partial b^{l}}$ should match the dimension of $W^{l}$ and $b^{l}$, respectively).<br><br>\n",
    "\n",
    "Plot the cost function for each iteration and compare the results after training with results from Problem 3. Discuss what you observe!\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your discussion here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Mini-Batch and Stochastic Gradient Descent [15pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "As a bonus, train the neural network using mini-batch gradient descent with batch size = 64 and stochastic gradient descent (i.e., batch size = 1) for 1000 epoch using learning rate = 0.005. Plot the cost vs iteration for both cases and briefly discuss your observation!  \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student solution here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your discussion here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
