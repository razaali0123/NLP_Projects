{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2020 - Assignment 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sophia Student, 1234567* (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, December 2</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE**\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-1.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle upload. In case of questions, you can reach us via the piazza forum, or by email.<br><br>\n",
    "<b>For this assignment, do NOT use any external packages (NLTK or any others) EXCEPT where specified.</b>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "In this assignment, you will implement and work with a Naive Bayes classifier. (Note that for this exercise, you don't need to represent the input as a vector necessarily. You can directly look at the presence of words, and look up the class conditional likelihood.)\n",
    "<br>\n",
    "<br>\n",
    "We will use a Twitter dataset classified into \"hate speech\" and \"non hate speech\" (in our data, we have called these classes \"offensive\" and \"nonoffensive\" to avoid the charged and inaccurate term \"hate speech\"). First, load the data (we have provided the function for this):\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Training data: 12896\nTest data: 3250\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "def read_hate_tweets (annofile, jsonfile):\n",
    "    \"\"\"Reads in hate speech data.\"\"\"\n",
    "    all_data = {}\n",
    "    annos = {}\n",
    "    with open(annofile) as csvfile:\n",
    "        csvreader = csv.reader(csvfile, delimiter=',')\n",
    "        for row in csvreader:\n",
    "            if row[0] in annos:\n",
    "                # if duplicate with different rating, remove!\n",
    "                if row[1] != annos[row[0]]:\n",
    "                    del(annos[row[0]])\n",
    "            else:\n",
    "                annos[row[0]] = row[1]\n",
    "\n",
    "    tknzr = TweetTokenizer()\n",
    "                \n",
    "    with open(jsonfile) as jsonfile:\n",
    "        for line in jsonfile:\n",
    "            twtjson = json.loads(line)\n",
    "            twt_id = twtjson['id_str']\n",
    "            if twt_id in annos:\n",
    "                all_data[twt_id] = {}\n",
    "                all_data[twt_id]['offensive'] = \"nonoffensive\" if annos[twt_id] == 'none' else \"offensive\"\n",
    "                all_data[twt_id]['text_tok'] = tknzr.tokenize(twtjson['text'])\n",
    "\n",
    "    # split training and test data:\n",
    "    all_data_sorted = sorted(all_data.items())\n",
    "    items = [(i[1]['text_tok'],i[1]['offensive']) for i in all_data_sorted]\n",
    "    splititem = len(all_data)-3250\n",
    "    train_dt = items[:splititem]\n",
    "    test_dt = items[splititem:]\n",
    "    print('Training data:',len(train_dt))\n",
    "    print('Test data:',len(test_dt))\n",
    "\n",
    "    return(train_dt,test_dt)\n",
    "\n",
    "TWEETS_ANNO = 'NAACL_SRW_2016.csv'\n",
    "TWEETS_TEXT = 'NAACL_SRW_2016_tweets.json'\n",
    "\n",
    "(train_data,test_data) = read_hate_tweets(TWEETS_ANNO,TWEETS_TEXT)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Each item in our data consists of a tuple of the tweet text and its label (represented as a string). The tweet text has been tokenized and is represented as a list of words. We can look at an example item:\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(['At', 'this', 'rate', ',', \"I'm\", 'going', 'to', 'be', 'making', 'slides', 'for', 'a', 'keynote', 'in', 'my', 'car', 'as', 'I', 'drive', 'home', '.'], 'nonoffensive')\n"
     ]
    }
   ],
   "source": [
    "print(train_data[4387])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Evaluation [15 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "The first thing you're being asked to do is to provide evaluation functions for a classifier and a given labelled test set. Assume that the classifier has a `predict()` function that takes an item in the form of a list as above and predicts a class for that item. Write evaluation functions to compute the `accuracy` and `f_1` score for such a classifier. (To test your functions without having access to a real `predict()` function, you could simulate one that makes random predictions.)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(classifier, data):\n",
    "    \"\"\"Computes the accuracy of a classifier on reference data.\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The accuracy of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    for x, label in data:\n",
    "        score+= (classifier.predict(x) == label)\n",
    "    return score/len(data)\n",
    "\n",
    "def f_1(classifier, data):\n",
    "    \"\"\"Computes the F_1-score of a classifier on reference data.\n",
    "\n",
    "    Args:\n",
    "        classifier: A classifier.\n",
    "        data: Reference data.\n",
    "\n",
    "    Returns:\n",
    "        The F_1-score of the classifier on the test data, a float.\n",
    "    \"\"\"\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    tn = 0\n",
    "    fn = 0\n",
    "    for x, label in data:\n",
    "        ypred = classifier.predict(x)\n",
    "        if label == \"offensive\":\n",
    "            tp+=(label == ypred)\n",
    "            fn+=(label != ypred)\n",
    "        else:\n",
    "            tn+=(label == ypred)\n",
    "            fp+=(label != ypred)\n",
    "    prec = tp/(tp+fp)\n",
    "    rec = tp/(tp+fn)\n",
    "\n",
    "    return (2*prec*rec)/(prec+rec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Naive Bayes Classifier [35 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Next, implement the Naive Bayes classifier from scratch using the code skeleton below and the definitions from class.<br><br>\n",
    "\n",
    "Some requirements and notes for implementation:\n",
    "\n",
    "<ul>\n",
    "<li> You should allow for an arbitrary number of classes (in particular, you should not hard code the two classes needed for the given dataset). \n",
    "<li> The vocabulary of your classifier should be created dynamically from the training data. (The vocabulary is the set of all words that occur in the training data.).\n",
    "<li> Use additive smoothing with a provided parameter k. \n",
    "<li> You may encounter unknown words at test time. Since we're not allowed to \"peek\" into the test set, we will implement the following simple treatment: We will assume that we don't know anything about unknown words and that in particular, their presence does not tell us anything about which class a document should be assigned to. Therefore, we will not include them in the calculation of the (log) probabilities during prediction, under the assumption that their probability does not differ hugely between the different classes (probably not a correct assumption, but the best we can do at this point). Since we don't need correct probabilities but only most likely classes, just ignore unknown words during prediction.\n",
    "<li> Use log probabilities in order to avoid underflow.\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[[-0.8073549220576042], [-2.807354922057604], [-1.8073549220576042]]"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "lst = [4,1,2]\n",
    "[[math.log2(num/sum(lst))] for num in lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "lst = [-1, -3, -7]\n",
    "val = sorted(lst)[-1]\n",
    "lst.index(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes(object):\n",
    "    \n",
    "    def __init__(self, num_classes, classes, dicts, totals, vocabsize, k, num_examples, vocab):\n",
    "        \"\"\"Initialises a new classifier.\"\"\"\n",
    "        self.num_classes = num_classes\n",
    "        self.classes = classes\n",
    "        self.dicts = dicts\n",
    "        self.totals = totals\n",
    "        self.vocabsize = vocabsize\n",
    "        self.k = k\n",
    "        self.num_examples = num_examples\n",
    "        self.vocab = vocab\n",
    "        pass\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"Predicts the class for a document.\n",
    "\n",
    "        Args:\n",
    "            x: A document, represented as a list of words.\n",
    "\n",
    "        Returns:\n",
    "            The predicted class, represented as a string.\n",
    "        \"\"\"\n",
    "        log_prob = [[math.log2(num/sum(self.num_examples))] for num in self.num_examples]\n",
    "        for w in x:\n",
    "            for cnt, d in enumerate(self.dicts):\n",
    "                if w in self.vocab:\n",
    "                    val = d.get(w,0)\n",
    "                    log_prob[cnt].append(math.log2((val + self.k)/(self.totals[cnt] + self.vocabsize)))\n",
    "        probs = [sum(p) for p in log_prob]\n",
    "        # print(\"probs are \", probs)\n",
    "        idx = probs.index(sorted(probs)[-1])\n",
    "\n",
    "        return self.classes[idx]\n",
    "        \n",
    "    @classmethod\n",
    "    def train(cls, data, k = 1):\n",
    "        \"\"\"Train a new classifier on training data using maximum\n",
    "        likelihood estimation and additive smoothing.\n",
    "\n",
    "        Args:\n",
    "            cls: The Python class representing the classifier.\n",
    "            data: Training data.\n",
    "            k: The smoothing constant.\n",
    "\n",
    "        Returns:\n",
    "            A trained classifier, an instance of `cls`.\n",
    "        \"\"\"\n",
    "        num_classes = len(set(list(zip(*data))[1]))\n",
    "        classes = list(set(list(zip(*data))[1]))\n",
    "        dicts = [{} for _ in range(num_classes)]\n",
    "        num_examples = [0 for _ in range(num_classes)]\n",
    "        for word, label in data:\n",
    "            ii = classes.index(label)\n",
    "            num_examples[ii]+=1\n",
    "            for w in word:\n",
    "                if w in dicts[ii].keys():\n",
    "                    dicts[ii][w]+=1\n",
    "                else:\n",
    "                    dicts[ii][w] = 1\n",
    "        totals = []\n",
    "        overall = []\n",
    "        for cnt, dct in enumerate(dicts):\n",
    "            overall+=dct.keys()\n",
    "            totals.append(sum(dct.values()))\n",
    "        vocab = set(overall)\n",
    "        vocabsize = len(set(overall))\n",
    "        # print(\"vocab size\", vocabsize)\n",
    "        # print(\"num classes\", num_classes)\n",
    "        # print(\"dicts\", dicts)\n",
    "        return cls(num_classes, classes, dicts, totals, vocabsize, k, num_examples, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Evaluate your classifier by training and testing it on the given data. Vary the smoothing parameter k. What happens when you decrease k? Plot a graph of the accuracy and/or f-score given different values of k. Discuss your findings.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  0.8572307692307692\nF_1:  0.5156576200417536\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "nb = NaiveBayes.train(train_data)\n",
    "print(\"Accuracy: \",accuracy(nb, test_data))\n",
    "print(\"F_1: \", f_1(nb,test_data))\n",
    "\n",
    "# TODO: test further smoothing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtr = [([\"chin\",\"chin\",\"tok\"], 'c'), (['chin',\"kar\", \"chin\"], 'c'), (['tok', \"tok\", \"kar\"], 't')]\n",
    "xte = [(['chin', 'tok', 'kar'], 'c')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy:  1.0\n"
     ]
    }
   ],
   "source": [
    "nb = NaiveBayes.train(xtr)\n",
    "print(\"Accuracy: \",accuracy(nb, xte))\n",
    "#print(\"F_1: \", f_1(nb,test_data))\n",
    "\n",
    "# TODO: test further smoothing parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Feature Engineering [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "We mentioned that the Naive Bayes classifier can be used with many different feature types. Try to improve on the basic bag of words model by changing the feature list of your model. Implement at least two variants. For each, explain your motivation for this feature set, and test the classifier with the given data. Briefly discuss your results!<br><br> \n",
    "Ideas for feature sets that were mentioned in class include:\n",
    "\n",
    "<ul>\n",
    "<li> removing stop words or frequent words\n",
    "<li> stemming or lemmatizing (you can use NLTK or spacy.io for basic NLP operations on the texts)\n",
    "<li> introducing part of speech tags as features (how?)\n",
    "<li> bigrams\n",
    "</ul>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Insert your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Logistic Regression Classifier [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Implement a logistic regression classifier using the definitions given in class and gradient descent. For this, you will have to use a matrix representation for your data to keep track of each feature's weights per class, which you can implement using the `numpy` package. <br><br> \n",
    "Start by implementing a function `featurize()` that converts the (training or testing) data into a matrix format. This function should return a pair of NumPy matrices 𝑿, 𝒀, where 𝑿 is an 𝑁 × 𝐹 matrix (𝑁: number of data instances, 𝐹: number of features), and where 𝒀 is an 𝑁 × 2 matrix whose rows have one of two forms:<br><br>\n",
    "[1, 0] if the gold-standard annotation class for the corresponding tweet is ‘offensive’, or <br><br>\n",
    "[0, 1] if the gold-standard class for the corresponding document is ‘nonoffensive’<br><br>\n",
    "This kind of representation is known as a one-hot encoding. You can read the first vector as saying that ‘there is a 100% chance that the instance belongs to the “offensive” class and a 0% chance that it belongs to the “nonoffensive” class’, and similarly for the second vector. Note that these are the two extreme cases for the conditional probability distribution P(k|x) for class k and feature vector x.<br><br>\n",
    "To implement the `featurize()` function, you will need to assign to each word in the training set a unique integer index which will identify that component of the feature vector which is 1 if the corresponding word is present in the document, and 0 otherwise. This index is built by the helper function `build_w2i()`.<br><br>\n",
    "Your next task is to complete the implementation of the `LogReg` class. The methods `p()` and `predict()` yield the probability of a class given an item, and the best class for the item, respectively. They can be implemented using appropriate NumPy matrix operations and the provided `softmax()` function. Note that you should set up both methods to take a whole matrix of input vectors as input, not just a single vector.<br><br>\n",
    "The training procedure is implemented in the (class) method `train()`, using iterative optimization. Typically, we shuffle the training data and split them into mini-batches (e.g, 100 items), then update the weights after each minibatch. This is done for `max_iter` number of iterations, or \"epochs\". Each epoch iterates over the training data set once.<br><br>\n",
    "Implement the missing methods using l_2 regularization with parameter C=0.1\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LogReg:\n",
    "    def __init__(self, eta=0.01, num_iter=30, step_size = 0.1):\n",
    "        self.eta = eta\n",
    "        self.num_iter = num_iter\n",
    "        self.step_size = step_size\n",
    "    \n",
    "\n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Calculate the softmax for the give inputs (array)\n",
    "        :param inputs:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return np.exp(inputs)/(np.sum(np.exp(inputs), axis=1).reshape(-1,1))\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "\n",
    "        # weights initialization\n",
    "        self.weights = np.zeros((X.shape[1], Y.shape[1]))\n",
    "        \n",
    "        for i in range(self.num_iter):\n",
    "            ii = np.random.permutation(X.shape[0])\n",
    "            X = X[ii,:]\n",
    "            Y = Y[ii,:]\n",
    "            for j in range(int(X.shape[0]/100) + 1):\n",
    "                xmini = X[j*100: j*100 + 100]\n",
    "                ymini = Y[j*100: j*100 + 100]\n",
    "                prod = np.matmul(xmini, self.weights)\n",
    "                o = self.softmax(prod)\n",
    "                loss = -1* np.sum(np.sum(np.multiply(np.log(o), ymini), axis= 1))\n",
    "                grad = (1/(xmini.shape[0])) * np.matmul(xmini.T, (o - ymini))\n",
    "                self.weights-= self.step_size*grad\n",
    "            if i%5 == 0:\n",
    "                print(f\"For the interation {i}, The loss is {loss}\")\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def featurize(self, data, training = True):\n",
    "        lst, label = list(zip(*data))\n",
    "        self.classes = list(set(label))\n",
    "        if training:\n",
    "            self.vocab = []\n",
    "            for doc in lst:\n",
    "                self.vocab+=doc\n",
    "            self.vocab = np.asarray(list(set(self.vocab)))\n",
    "        xtr = np.zeros((len(lst), len(self.vocab)))\n",
    "        ytr = np.zeros((len(lst), len(self.classes)))\n",
    "        for c, (doc, lbl) in enumerate(zip(lst, label)):\n",
    "            xtr[c, self.build_w2i(self.vocab, np.asarray(doc))] = 1\n",
    "            ytr[c, self.classes.index(lbl)] = 1\n",
    "        return xtr, ytr\n",
    "    \n",
    "    def build_w2i(self, vocab, doc):\n",
    "        return np.isin(vocab, doc)\n",
    "    \n",
    "    \n",
    "    def p(self, X): \n",
    "        X, _ = self.featurize(X, training= False)\n",
    "        return self.softmax(np.matmul(X, self.weights))\n",
    "        \n",
    "    \n",
    "    def predict(self, X):\n",
    "        prob = self.p(X)\n",
    "        ii = np.argmax(prob, axis = 1).reshape(-1,1)\n",
    "        return np.asarray(self.classes)[ii]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "Test your implementation using 10 iterations, default learning rate eta, and l_2 regularization with parameter C=0.1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "For the interation 0, The loss is 54.630343891018825\n",
      "For the interation 5, The loss is 33.084464618152495\n",
      "For the interation 10, The loss is 38.553292099518536\n",
      "For the interation 15, The loss is 31.288455463676698\n",
      "For the interation 20, The loss is 30.067981746616688\n",
      "For the interation 25, The loss is 35.39171241884448\n"
     ]
    }
   ],
   "source": [
    "logclass = LogReg(num_iter= 30, step_size= 0.3)\n",
    "xtr, ytr = logclass.featurize(train_data)\n",
    "logclass.train(xtr, ytr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "_,yte = logclass.featurize(test_data, training= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred = logclass.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob = logclass.p(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.13524571, -2.06752289],\n",
       "       [-0.29482681, -1.36516141],\n",
       "       [-0.0640862 , -2.77939818],\n",
       "       [-0.14546562, -1.99966677],\n",
       "       [-0.08597697, -2.49635627]])"
      ]
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "prob[:5,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['offensive']], dtype='<U9')"
      ]
     },
     "metadata": {},
     "execution_count": 105
    }
   ],
   "source": [
    "eg = [(['fuck', 'you', 'Macron', 'go', 'to', 'hell'], \"offensive\")]\n",
    "logclass.predict(eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python36964bitanlpvenv888d8570ac234479958570a9853e5ed6",
   "display_name": "Python 3.6.9 64-bit ('anlp': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}