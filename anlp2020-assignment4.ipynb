{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANLP 2020 - Assignment 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Sophia Student, 1234567* (enter your name/student id number here)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">Due: Wednesday, February 3rd 2021</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**NOTE**<br><br>\n",
    "\n",
    "Please first fill in your name and id number at the top of the assignment, and **rename** the assignment file to **yourlastname-anlp-4.ipynb**<br><br>\n",
    "Problems and questions are given in blue boxes like this one. All grey and white boxes must be filled by you (they either require code or a (brief!) discussion). <br><br>\n",
    "Please hand in your assignment by the deadline via Moodle upload (we will provide a link). In case of questions, you can contact us via email, or via the Piazza (preferred).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural ambiguity [20 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "Devise at least two sentences that exhibit structural ambiguity (i.e., ambiguity that comes from different structural analyses, not from ambiguity in the meaning of an individual lexical item). Indicate the different analyses (at least two per sentence) with a syntactic tree.\n",
    "\n",
    "You can draw trees by using the NLTK package:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package large_grammars to /home/local/MPIB-\n[nltk_data]     BERLIN/raza/nltk_data...\n[nltk_data]   Package large_grammars is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tree import Tree\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "nltk.download('large_grammars')\n",
    "import configparser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "Tree('S', [Tree('NP', [Tree('She', [])]), Tree('VP', [Tree('ran', [])])])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree.fromstring('(S (NP (She)) (VP (ran)))')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CKY Parsing. Some Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "In this assignment, you will implement the CKY algorithm for English and apply it to the word recognition and parsing problem. You can use the NLTK modules for representing context-free grammars and parse trees, but you should implement the parser from scratch. (But you don't *have* to use `nltk.grammar` if you find it easier without. Some NLTK modules are overly complicated, and in this assignment you mainly need a lookup function from right-hand-sides of grammar rules to left-hand-sides.)<br><br>\n",
    "\n",
    "We provide the grammar and the test sentences.\n",
    "The grammar stems from the Airline Travel Information System (ATIS), a project working on spoken dialog systems for air travel. (What do you think, is this a grammar motivated by syntactic theory, or perhaps more motivated by other concerns? Does it look like the sample grammars we've seen in class, only larger, or does it fundamentally look different?) The ATIS CFG is available in the NLTK data package, together with 98 test sentences. You can initialize the resources this way:\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the grammar\n",
    "grammar = nltk.data.load(\"grammars/large_grammars/atis.cfg\")\n",
    "# load the raw sentences\n",
    "s = nltk.data.load(\"grammars/large_grammars/atis_sentences.txt\", \"auto\")\n",
    "# extract the test sentences\n",
    "t = nltk.parse.util.extract_test_sentences(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "NLTK already implements a number of parsing algorithms (see `nltk.parse` for the list). You can try one to see if you loaded the grammar correctly.<br><br>\n",
    "Note that NLTK's `chart_parse()` throws an error when it encounters an unknown word, which is undesirable behavior for any parser. If you want to test the grammars with the pre-implemented parser (for example, to check whether the CNF version is in fact weakly equivalent), you may need to catch this error.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the parser\n",
    "parser = nltk.parse.BottomUpChartParser(grammar)\n",
    "# parse all test sentences\n",
    "for sentence in t[:10]:\n",
    "    parser.chart_parse(sentence[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "However, the NLTK version of the ATIS grammar is not in Chomsky normal form (CNF), which you will need for your CKY parser. Feel free to implement a conversion module for extra credit, but for your convenience, we have already converted the ATIS CFG into CNF, provided with this assignment. You can then read the grammar from the file using `nltk.data.load()` and utilize the features of the `nltk.grammar` module on the resulting object.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Word Recognition [50 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Implement the CKY algorithm and use it as a recognizer. That is, given an input sentence, the procedure should decide whether the sentence is in the language of the CFG or not. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        A                       B\n",
       "0  SIGMA              ADJ_ABL JYL\n",
       "1    GCC               AVP_RB GCD\n",
       "2    GCD       NP_DTS pt_char_per\n",
       "3    GCA               AVP_RB GCB\n",
       "4    GCB   COMPCL_VBZ pt_char_per"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>A</th>\n      <th>B</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>SIGMA</td>\n      <td>ADJ_ABL JYL</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>GCC</td>\n      <td>AVP_RB GCD</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>GCD</td>\n      <td>NP_DTS pt_char_per</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>GCA</td>\n      <td>AVP_RB GCB</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>GCB</td>\n      <td>COMPCL_VBZ pt_char_per</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 237
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/atis-grammar-cnf.cfg\", sep = '-> ', header= None, names= ['A', 'B'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SIGMA '"
      ]
     },
     "metadata": {},
     "execution_count": 208
    }
   ],
   "source": [
    "df['A'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 226
    }
   ],
   "source": [
    "df['B'][16500] == \"airplane\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "A    NP_NP\n",
       "B     None\n",
       "Name: 6802, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 236
    }
   ],
   "source": [
    "df.iloc[6802]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['A'] = df['A'].apply(lambda x: x[:-1])\n",
    "df['B'] = df['B'].apply(lambda x: x[1:-1] if x[0]=='\"' else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['jamal', 'khan', 'memor', 'kk']"
      ]
     },
     "metadata": {},
     "execution_count": 256
    }
   ],
   "source": [
    "a = \"jamal khan      memor                       kk\" + \" \"\n",
    "a.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup(aa,bb):\n",
    "    aa = aa.split()\n",
    "    bb = bb.split()\n",
    "    idx = \"\"\n",
    "    for a in aa:\n",
    "        for b in bb:\n",
    "            look = a+\" \"+b\n",
    "            idx += \" \".join(df[df['B'] == look]['A'].tolist()) \n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['hello', 'hello', 'hello', 'hello', 'hello'],\n",
       "       ['hello', 'hello', 'hello hhh', 'hello', 'hello'],\n",
       "       ['hello', 'hello', 'hello', 'hello', 'hello'],\n",
       "       ['hello', 'hello', 'hello', 'hello', 'hello'],\n",
       "       ['hello', 'hello', 'hello', 'hello', 'hello']], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 295
    }
   ],
   "source": [
    "n = 3\n",
    "chart = np.zeros((n+2,n+2), dtype = \"object\")\n",
    "chart[:,:]=\"hello\"\n",
    "chart[1,2] = chart[1,2] + \" \"+ \"hhh\"\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['jj']"
      ]
     },
     "metadata": {},
     "execution_count": 298
    }
   ],
   "source": [
    "(\" \"+\"jj\").split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here\n",
    "def cky(sentence, df= df):\n",
    "    words = sentence.split()\n",
    "    n = len(words)\n",
    "    chart = np.zeros((n+2,n+2), dtype = \"object\")\n",
    "    chart[:,:] = \"\"\n",
    "    for i, w in enumerate(words):\n",
    "        idx = df[df['B'] == w]['A'].tolist()\n",
    "        chart[i+1, i+2] = \" \".join(idx)\n",
    "    \n",
    "\n",
    "    #\n",
    "    for b in range(2, n+1):\n",
    "        for i in range(1, n-b+2):\n",
    "            for k in range(1,b):\n",
    "                chart[i, i+b] = chart[i, i+b] + \" \" + lookup(chart[i,i+k], chart[i+k, i+b])\n",
    "\n",
    "    if \"SIGMA\" in chart[1,n+1].split():\n",
    "        print(\"SENTENCE CAN BE MADE USING VOCABLARY\")\n",
    "    else:\n",
    "        print(\"Sorry, the sentence cannot be made using this vocablary\")\n",
    "    return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sorry, the sentence cannot be made using this vocablary\n"
     ]
    }
   ],
   "source": [
    "#chart = cky(\"get the hell out of here\")\n",
    "#chart = cky(\"are you free\")\n",
    "#chart = cky(\"john live in city\")\n",
    "chart = cky(\"a good day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['', '', '', '', ''],\n",
       "       ['', '',\n",
       "        'NOUN_NP ADJ_AT NP_NP a SIGMA AVPNP_NP PREP_IN NAPPOS_NP', ' ',\n",
       "        '  '],\n",
       "       ['', '', '', '', ' '],\n",
       "       ['', '', '', '', 'NOUN_NN NP_NN SIGMA AVPNP_NN pt120'],\n",
       "       ['', '', '', '', '']], dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 305
    }
   ],
   "source": [
    "chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Provide a list of grammatical and ungrammatical test sentences (at least 10 each) and test your recognizer on these sentences. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your code/discussion goes here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Parsing [30 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Now extend your CKY recognizer into a parser by adding backpointers. Also implement a function that extracts the set of all parse trees from the backpointers in the chart. Feel free to use the NLTK module `nltk.tree` for this purpose; notice that only `ImmutableTree`s can be used as elements of Python sets, whereas raw `Tree`s cannot.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Test the parser by providing the list of ATIS test sentences with tab-separated numbers of parse trees.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your code/results go here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "Choose an ATIS test sentence with a number of parses $p$ such that $1 < p < 5$. Provide pictures of its parses. You can visualize an NLTK tree using its `draw` method. Discuss the structural differences.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*your code/results go here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Credit [10 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "If you still have time left, you can attempt the following project for extra credit. Perhaps it has occurred to you that it is quite wasteful to compute all parse trees just to find out how many parse trees there are. Figure out how to compute the number of parse trees for an entry $A ∈ Ch(i, k)$ from your chart with backpointers, without actually computing these parse trees. Verify that you get the correct results, and compare the efficiency of your new procedure to your earlier solution.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code goes here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "alt-ctrl-e"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}